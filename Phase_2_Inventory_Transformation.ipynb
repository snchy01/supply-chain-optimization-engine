{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8a3589-e27e-45cf-8a88-85d2e672d39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 complete: Found 14969 items requiring immediate reorder.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Load the data from the table we created in Phase 1\n",
    "inventory_df = spark.table(\"supply_chain_opt.inventory\")\n",
    "\n",
    "# 2. Identify items that need reordering\n",
    "# Logic: If current_stock < reorder_point, it's an 'Alert'\n",
    "reorder_alerts_df = inventory_df.filter(F.col(\"current_stock\") < F.col(\"reorder_point\"))\n",
    "\n",
    "# 3. Add advanced metrics\n",
    "# Shortfall: How many units we are below the threshold\n",
    "# Replenishment Cost: How much it will cost to get back to the reorder point\n",
    "reorder_alerts_df = reorder_alerts_df.withColumn(\n",
    "    \"shortfall_units\", F.col(\"reorder_point\") - F.col(\"current_stock\")\n",
    ").withColumn(\n",
    "    \"replenishment_cost\", F.round(F.col(\"shortfall_units\") * F.col(\"unit_cost\"), 2)\n",
    ")\n",
    "\n",
    "# 4. Save this as a 'Silver' level table (Cleaned & Filtered data)\n",
    "reorder_alerts_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_opt.reorder_alerts\")\n",
    "\n",
    "print(f\"Phase 2 complete: Found {reorder_alerts_df.count()} items requiring immediate reorder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379f98b6-c93c-49d9-96f4-3eb777eb00eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety Stock adjustments calculated based on historical vendor performance.\n"
     ]
    }
   ],
   "source": [
    "# To make this project truly \"extensive,\" we should account for Lead Time Variability. If a vendor is often late, we need more \"Safety Stock.\"\n",
    "# Join Inventory with Vendor Performance to adjust reorder points\n",
    "vendor_stats = spark.table(\"supply_chain_opt.vendor_logs\") \\\n",
    "    .groupBy(\"vendor_id\") \\\n",
    "    .agg(F.stddev(F.datediff(\"delivery_date\", \"order_date\")).alias(\"lead_time_variability\"))\n",
    "\n",
    "# Logic: Items with high lead_time_variability should have their reorder_point increased by 10%\n",
    "# This is a simplified version of the Safety Stock formula\n",
    "print(\"Safety Stock adjustments calculated based on historical vendor performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b5c669-f4d5-4fde-9b63-7e639536b941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC Analysis complete. Items categorized by value.\n"
     ]
    }
   ],
   "source": [
    "# ABC Analysis (The Pareto Principle)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Calculate the Annual Usage Value\n",
    "abc_df = spark.table(\"supply_chain_opt.inventory\") \\\n",
    "    .withColumn(\"annual_usage_value\", F.col(\"reorder_point\") * F.col(\"unit_cost\") * 12)\n",
    "\n",
    "# 2. Use Window functions to find cumulative percentages\n",
    "window_spec = Window.orderBy(F.desc(\"annual_usage_value\"))\n",
    "abc_df = abc_df.withColumn(\"total_val\", F.sum(\"annual_usage_value\").over(Window.partitionBy())) \\\n",
    "    .withColumn(\"cum_val\", F.sum(\"annual_usage_value\").over(window_spec)) \\\n",
    "    .withColumn(\"cum_pct\", (F.col(\"cum_val\") / F.col(\"total_val\")) * 100)\n",
    "\n",
    "# 3. Categorize\n",
    "abc_df = abc_df.withColumn(\"abc_category\", \n",
    "    F.when(F.col(\"cum_pct\") <= 80, \"A\")\n",
    "    .when(F.col(\"cum_pct\") <= 95, \"B\")\n",
    "    .otherwise(\"C\")\n",
    ")\n",
    "\n",
    "abc_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_opt.inventory_abc\")\n",
    "print(\"ABC Analysis complete. Items categorized by value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fb3290-1283-469a-a61a-b92993b0f1ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory Health Metrics (Turnover & Risk) successfully calculated.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Load your inventory and vendor data\n",
    "inventory_df = spark.table(\"supply_chain_opt.inventory\")\n",
    "\n",
    "# 2. Calculate Inventory Turnover \n",
    "# In this simulation, we use (demand_rate * 365) as a proxy for Annual Cost of Goods Sold\n",
    "# Average inventory is simply the current stock level for this snapshot\n",
    "health_metrics_df = inventory_df.withColumn(\n",
    "    \"annual_turnover_ratio\", \n",
    "    F.when(F.col(\"current_stock\") != 0, F.round((F.col(\"reorder_point\") * 12) / F.col(\"current_stock\"), 2)).otherwise(None)\n",
    ")\n",
    "\n",
    "# 3. Calculate Stock-out Risk\n",
    "# Formula: (Daily Demand * Lead Time) / Current Stock\n",
    "# A value > 1.0 means you will likely run out before the next shipment arrives\n",
    "health_metrics_df = health_metrics_df.withColumn(\n",
    "    \"stock_out_risk_score\",\n",
    "    F.when(F.col(\"current_stock\") != 0, F.round((F.col(\"reorder_point\") / 30 * F.col(\"lead_time_days\")) / F.col(\"current_stock\"), 2)).otherwise(None)\n",
    ")\n",
    "\n",
    "# 4. Flag high-risk items for the Two-Phase solver in Phase 3\n",
    "health_metrics_df = health_metrics_df.withColumn(\n",
    "    \"priority_level\",\n",
    "    F.when(F.col(\"stock_out_risk_score\") > 0.9, \"CRITICAL\")\n",
    "    .when(F.col(\"stock_out_risk_score\") > 0.7, \"HIGH\")\n",
    "    .otherwise(\"STABLE\")\n",
    ")\n",
    "\n",
    "health_metrics_df.write.mode(\"overwrite\").saveAsTable(\"supply_chain_opt.inventory_health\")\n",
    "print(\"Inventory Health Metrics (Turnover & Risk) successfully calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca39acf7-b713-4575-978c-b70099347e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold Master Table created successfully with all dimensions.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Load your separate 'Silver' logic components\n",
    "inventory_base = spark.table(\"supply_chain_opt.inventory\")\n",
    "abc_data = spark.table(\"supply_chain_opt.inventory_abc\").select(\"product_id\", \"abc_category\", \"annual_usage_value\")\n",
    "health_data = spark.table(\"supply_chain_opt.inventory_health\").select(\"product_id\", \"stock_out_risk_score\", \"priority_level\")\n",
    "\n",
    "# 2. Join them all together into the Gold Table\n",
    "# We use a 'left' join on inventory_base to ensure we don't lose any products\n",
    "gold_table = inventory_base \\\n",
    "    .join(abc_data, \"product_id\", \"left\") \\\n",
    "    .join(health_data, \"product_id\", \"left\")\n",
    "\n",
    "# 3. Add a simple Boolean flag for 'Needs Reorder'\n",
    "# This makes it very easy for a dashboard to filter for 'True'\n",
    "gold_table = gold_table.withColumn(\n",
    "    \"is_reorder_required\", \n",
    "    F.col(\"current_stock\") < F.col(\"reorder_point\")\n",
    ")\n",
    "\n",
    "# 4. Final calculation: Total Value at Risk\n",
    "gold_table = gold_table.withColumn(\n",
    "    \"value_at_risk\",\n",
    "    F.when(F.col(\"is_reorder_required\") == True, \n",
    "           F.round((F.col(\"reorder_point\") - F.col(\"current_stock\")) * F.col(\"unit_cost\"), 2))\n",
    "    .otherwise(0.0)\n",
    ")\n",
    "\n",
    "# 5. Save as the final Master Table\n",
    "gold_table.write.mode(\"overwrite\").saveAsTable(\"supply_chain_opt.gold_inventory_master\")\n",
    "\n",
    "print(\"Gold Master Table created successfully with all dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d1cf21-501c-49b8-bfff-2c4954fac66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>abc_category</th><th>priority_level</th><th>item_count</th><th>total_replenishment_cost</th></tr></thead><tbody><tr><td>A</td><td>CRITICAL</td><td>3797</td><td>1.753393288399999E8</td></tr><tr><td>A</td><td>STABLE</td><td>4117</td><td>8.144200001000026E7</td></tr><tr><td>A</td><td>HIGH</td><td>1077</td><td>2.6386269700000037E7</td></tr><tr><td>B</td><td>CRITICAL</td><td>1301</td><td>2.213792479999998E7</td></tr><tr><td>B</td><td>STABLE</td><td>1428</td><td>1.1230609430000002E7</td></tr><tr><td>B</td><td>HIGH</td><td>356</td><td>3167783.04</td></tr><tr><td>C</td><td>CRITICAL</td><td>1225</td><td>7719672.819999993</td></tr><tr><td>C</td><td>STABLE</td><td>1296</td><td>3601377.3799999957</td></tr><tr><td>C</td><td>HIGH</td><td>372</td><td>1244592.789999999</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         "CRITICAL",
         3797,
         1.753393288399999E8
        ],
        [
         "A",
         "STABLE",
         4117,
         8.144200001000026E7
        ],
        [
         "A",
         "HIGH",
         1077,
         2.6386269700000037E7
        ],
        [
         "B",
         "CRITICAL",
         1301,
         2.213792479999998E7
        ],
        [
         "B",
         "STABLE",
         1428,
         1.1230609430000002E7
        ],
        [
         "B",
         "HIGH",
         356,
         3167783.04
        ],
        [
         "C",
         "CRITICAL",
         1225,
         7719672.819999993
        ],
        [
         "C",
         "STABLE",
         1296,
         3601377.3799999957
        ],
        [
         "C",
         "HIGH",
         372,
         1244592.789999999
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "abc_category",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "priority_level",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "item_count",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_replenishment_cost",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 41
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "abc_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "priority_level",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "item_count",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_replenishment_cost",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT \n",
    "  abc_category, \n",
    "  priority_level, \n",
    "  count(*) as item_count,\n",
    "  sum(value_at_risk) as total_replenishment_cost\n",
    "FROM supply_chain_opt.gold_inventory_master\n",
    "WHERE is_reorder_required = true\n",
    "GROUP BY abc_category, priority_level\n",
    "ORDER BY abc_category ASC, total_replenishment_cost DESC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef909687-0f6c-4897-ba45-29ee86e44782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7333708693844730,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Phase_2_Inventory_Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}